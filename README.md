# ğŸš€ Pipeline de Datos E-commerce - DataOps

## Parcial Segundo Corte - ImplementaciÃ³n Completa

**Autor:** [Tu Nombre]  
**Curso:** DataOps  
**Fecha:** Octubre 2024  
**GitHub:** [tu-usuario/ecommerce-pipeline](https://github.com/tu-usuario/ecommerce-pipeline)

---

## ğŸ“‹ Tabla de Contenidos

1. [Ejercicio 1: DiseÃ±o de Pipeline](#ejercicio-1-diseÃ±o-de-pipeline-25-puntos)
2. [Ejercicio 2: ImplementaciÃ³n con Python](#ejercicio-2-implementaciÃ³n-con-python-35-puntos)
3. [InstalaciÃ³n y ConfiguraciÃ³n](#instalaciÃ³n-y-configuraciÃ³n)
4. [Uso del Pipeline](#uso-del-pipeline)
5. [Estructura del Proyecto](#estructura-del-proyecto)
6. [Ejemplos de Salida](#ejemplos-de-salida)
7. [TecnologÃ­as Utilizadas](#tecnologÃ­as-utilizadas)

---

## Ejercicio 1: DiseÃ±o de Pipeline (25 puntos)

### ğŸ¯ Contexto del Negocio

Una startup de e-commerce necesita integrar y analizar datos provenientes de mÃºltiples fuentes:

**Fuentes de Datos:**
- ğŸŒ **API REST** - CatÃ¡logo de productos en tiempo real (Fake Store API)
- ğŸ“„ **CSV de Ventas** - HistÃ³rico de transacciones
- ğŸ“¦ **CSV de Inventario** - Stock actual y mÃ­nimos requeridos

**Requisitos de Negocio:**
- âœ… Procesamiento diario automatizado
- ğŸ“Š Dashboard de anÃ¡lisis de productos
- âš ï¸ Alertas de stock crÃ­tico
- ğŸ’° CÃ¡lculo de mÃ©tricas de rentabilidad

---

### 1ï¸âƒ£ Diagrama de Arquitectura del Pipeline (10 puntos)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                          CAPA DE INGESTA                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                     â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚   â”‚ Fake Store  â”‚      â”‚   Sales     â”‚      â”‚  Inventory  â”‚      â”‚
â”‚   â”‚  API REST   â”‚      â”‚  CSV File   â”‚      â”‚  CSV File   â”‚      â”‚
â”‚   â”‚ (Streaming) â”‚      â”‚  (Batch)    â”‚      â”‚  (Batch)    â”‚      â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚          â”‚                    â”‚                    â”‚              â”‚
â”‚          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
â”‚                               â”‚                                    â”‚
â”‚                    [requests + pandas]                            â”‚
â”‚                               â”‚                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    CAPA DE ALMACENAMIENTO RAW                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                     â”‚
â”‚                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                        â”‚
â”‚                â”‚  Data Lake (Parquet)     â”‚                        â”‚
â”‚                â”‚  âœ“ products.parquet      â”‚                        â”‚
â”‚                â”‚  âœ“ sales.parquet         â”‚                        â”‚
â”‚                â”‚  âœ“ inventory.parquet     â”‚                        â”‚
â”‚                â”‚                          â”‚                        â”‚
â”‚                â”‚  Ventajas:               â”‚                        â”‚
â”‚                â”‚  â€¢ CompresiÃ³n ~70%       â”‚                        â”‚
â”‚                â”‚  â€¢ Columnar storage      â”‚                        â”‚
â”‚                â”‚  â€¢ Preserva tipos        â”‚                        â”‚
â”‚                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                        â”‚
â”‚                             â”‚                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚               CAPA DE PROCESAMIENTO Y TRANSFORMACIÃ“N                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                     â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚   â”‚          Transformaciones (Pandas)                       â”‚    â”‚
â”‚   â”‚  â€¢ JOIN: sales â† inventory â† products                    â”‚    â”‚
â”‚   â”‚  â€¢ NormalizaciÃ³n de columnas                             â”‚    â”‚
â”‚   â”‚  â€¢ CÃ¡lculo de mÃ©tricas de negocio:                       â”‚    â”‚
â”‚   â”‚    - Valor total de ventas                               â”‚    â”‚
â”‚   â”‚    - Rentabilidad por producto                           â”‚    â”‚
â”‚   â”‚    - Ventas totales por categorÃ­a                        â”‚    â”‚
â”‚   â”‚  â€¢ Agregaciones y rankings                               â”‚    â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                        â”‚                                           â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚   â”‚          Tests de Calidad de Datos                       â”‚    â”‚
â”‚   â”‚  âœ“ Precios no negativos                                  â”‚    â”‚
â”‚   â”‚  âœ“ Stock vÃ¡lido (entero >= 0)                            â”‚    â”‚
â”‚   â”‚  âœ“ CategorÃ­as no nulas                                   â”‚    â”‚
â”‚   â”‚  âœ“ Fechas vÃ¡lidas                                        â”‚    â”‚
â”‚   â”‚                                                           â”‚    â”‚
â”‚   â”‚  Enfoque: Shift-Left Testing                             â”‚    â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                        â”‚                                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 CAPA DE ALMACENAMIENTO PROCESADO                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                     â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
â”‚   â”‚  Resultados AnalÃ­ticos (CSV/Parquet)          â”‚               â”‚
â”‚   â”‚  â€¢ datos_procesados.csv                       â”‚               â”‚
â”‚   â”‚  â€¢ stock_critico.csv                          â”‚               â”‚
â”‚   â”‚  â€¢ top_productos.csv                          â”‚               â”‚
â”‚   â”‚  â€¢ ventas_categoria.csv                       â”‚               â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
â”‚                         â”‚                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  CAPA DE ANÃLISIS Y REPORTES                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                     â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
â”‚   â”‚   Reportes   â”‚   â”‚   Alertas    â”‚   â”‚  Dashboard   â”‚         â”‚
â”‚   â”‚   (.txt)     â”‚   â”‚   (Email/    â”‚   â”‚  (Futuro:    â”‚         â”‚
â”‚   â”‚              â”‚   â”‚   Slack)     â”‚   â”‚  Streamlit)  â”‚         â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
â”‚                                                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    COMPONENTES TRANSVERSALES                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  ğŸ”§ OrquestaciÃ³n: Script Python (orchestador.py)                   â”‚
â”‚  ğŸ“ Logging: logging module (archivo + consola)                    â”‚
â”‚  âš™ï¸  ConfiguraciÃ³n: YAML (pipeline_config.yaml)                     â”‚
â”‚  ğŸ”„ Versionamiento: Git + GitHub                                    â”‚
â”‚  âœ… Calidad: Tests integrados en transformaciÃ³n                     â”‚
â”‚  ğŸ Lenguaje: Python 3.8+                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### ğŸ”‘ Componentes Clave

| Componente | TecnologÃ­a | JustificaciÃ³n |
|------------|-----------|---------------|
| **Ingesta Batch** | CSV â†’ Pandas | Archivos locales pequeÃ±os-medianos |
| **Ingesta API** | requests + JSON | Datos en tiempo real de productos |
| **Almacenamiento Raw** | Parquet | CompresiÃ³n, columnar, tipos preservados |
| **Procesamiento** | Pandas | Ecosistema maduro, sintaxis intuitiva |
| **Calidad** | Assertions + logging | Tests integrados en el pipeline |
| **Reportes** | TXT + CSV | Legibles, exportables, compatibles |
| **OrquestaciÃ³n** | Python scripts | Simplicidad, sin dependencias cloud |

---

### 2ï¸âƒ£ JustificaciÃ³n TÃ©cnica (15 puntos)

#### â“ Â¿Por quÃ© diseÃ±Ã© asÃ­ el pipeline?

**DecisiÃ³n 1: Arquitectura ELT vs ETL**

ElegÃ­ **ELT (Extract-Load-Transform)** porque:

âœ… **Ventajas:**
- **Inmutabilidad de datos crudos:** Los datos raw en Parquet nunca se modifican
- **Re-procesamiento sin re-ingesta:** Puedo transformar mÃºltiples veces sin descargar de nuevo
- **Flexibilidad:** Nuevas transformaciones no requieren cambiar la ingesta
- **AuditorÃ­a:** Siempre puedo revisar los datos originales

ğŸ“Š **Flujo ELT implementado:**
```
Extract (API/CSV) â†’ Load (Parquet) â†’ Transform (Pandas) â†’ Report (CSV/TXT)
```

**DecisiÃ³n 2: Formato Parquet para Almacenamiento**

| CaracterÃ­stica | CSV | Parquet | DecisiÃ³n |
|----------------|-----|---------|----------|
| CompresiÃ³n | âŒ No | âœ… ~70% | **Parquet** |
| Tipos de datos | âŒ Texto | âœ… Preservados | **Parquet** |
| Velocidad lectura | âŒ Lenta | âœ… RÃ¡pida (columnar) | **Parquet** |
| Compatibilidad | âœ… Universal | âš ï¸ Requiere librerÃ­a | **Parquet** (con PyArrow) |

**DecisiÃ³n 3: SeparaciÃ³n en Capas (Zones)**

```
Raw Zone â†’ Processing Zone â†’ Analytics Zone
```

- **Raw:** Datos inmutables, tal como llegaron
- **Processing:** Datos limpios y enriquecidos
- **Analytics:** Resultados listos para consumo

Esto facilita:
- ğŸ› **Debugging:** Revisar datos crudos cuando algo falla
- ğŸ”„ **Rollback:** Volver a procesar sin re-ingestar
- ğŸ“Š **AuditorÃ­a:** Trazabilidad completa del dato

---

#### â“ Â¿CÃ³mo garantizo la calidad de datos?

Implemento una **estrategia de calidad multinivel** basada en **Shift-Left Testing**:

**Nivel 1: Tests de Esquema**
```python
# Validar estructura esperada ANTES de procesar
required_cols = ['product_id', 'price', 'category']
if not all(col in df.columns for col in required_cols):
    raise ValueError("Esquema invÃ¡lido")
```

**Nivel 2: Tests de Integridad** (Implementados)
```python
âœ“ Precios no negativos: (df["price"] >= 0).all()
âœ“ Stock vÃ¡lido: df["stock"].apply(lambda x: isinstance(x, int) and x >= 0).all()
âœ“ CategorÃ­as no nulas: df["category"].notna().all()
âœ“ Fechas vÃ¡lidas: pd.to_datetime(df["date"], errors="raise")
```

**Nivel 3: Tests de Negocio**
```python
# Ejemplo: Rentabilidad coherente
assert (df["price"] > df["cost"]).all(), "Precio debe ser mayor al costo"
```

**Nivel 4: Logging y Alertas**
```python
# Registro de cada test ejecutado
logger.info(f"âœ“ Test de precios: {test_result}")
if not test_result:
    logger.warning(f"âš ï¸ {failed_records} registros con precios invÃ¡lidos")
```

**Principio Shift-Left:** Detectar problemas **lo mÃ¡s temprano posible** en el pipeline para evitar procesar datos corruptos.

---

#### â“ Â¿QuÃ© estrategia uso para versionamiento?

**Versionamiento de CÃ³digo (Git)**

```bash
# Estructura de branches
main          â† CÃ³digo en producciÃ³n (estable)
  â”œâ”€â”€ develop â† IntegraciÃ³n continua
  â”‚   â”œâ”€â”€ feature/ingestion-api
  â”‚   â”œâ”€â”€ feature/quality-checks
  â”‚   â””â”€â”€ feature/reporting
  â””â”€â”€ hotfix/fix-encoding-error
```

**Commits AtÃ³micos:**
```bash
âœ… BIEN:
git commit -m "feat: agregar mÃ³dulo de reportes"
git commit -m "fix: corregir encoding en logs UTF-8"
git commit -m "docs: actualizar README con ejemplos"

âŒ MAL:
git commit -m "cambios varios"
git commit -m "update"
```

**Versionamiento de Datos (DVC)**

Para datasets grandes, usarÃ­a **DVC (Data Version Control)**:

```bash
# Trackear datasets
dvc add data/sales.csv
dvc add data/inventory.csv

# Commit del archivo .dvc (ligero)
git add data/sales.csv.dvc
git commit -m "data: actualizar ventas Q4 2024"

# Push de datos a storage remoto
dvc push
```

**Ventajas de DVC:**
- ğŸ“¦ Versiona datasets como si fueran cÃ³digo
- â˜ï¸ Storage remoto (S3, GCS, Azure)
- ğŸ”„ Rollback de datos a cualquier versiÃ³n
- ğŸ¤ ColaboraciÃ³n en equipo

**Versionamiento de ConfiguraciÃ³n**

```yaml
# config/pipeline_config_v1.0.0.yaml
version: "1.0.0"
api:
  url: "https://fakestoreapi.com/products"
  timeout: 30
```

**Changelog:**
```markdown
## [1.1.0] - 2024-10-31
### Added
- MÃ³dulo de reportes con exportaciÃ³n CSV
- Test de fechas vÃ¡lidas

### Changed
- Mejorado manejo de errores en ingesta
```

---

#### â“ Â¿CÃ³mo manejo la escalabilidad?

**Estrategia de Escalamiento Progresivo**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Stage 1: PoC Local (ACTUAL)                            â”‚
â”‚  â€¢ Pandas + Python scripts                              â”‚
â”‚  â€¢ 10K-100K registros/dÃ­a                               â”‚
â”‚  â€¢ EjecuciÃ³n local o cron                               â”‚
â”‚  â€¢ Costo: $0                                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â–¼ Crece a 100K-1M registros
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Stage 2: OptimizaciÃ³n (6-12 meses)                     â”‚
â”‚  â€¢ Polars (10-50x mÃ¡s rÃ¡pido que Pandas)                â”‚
â”‚  â€¢ Airflow para orquestaciÃ³n                            â”‚
â”‚  â€¢ Particionamiento de datos                            â”‚
â”‚  â€¢ 100K-1M registros/dÃ­a                                â”‚
â”‚  â€¢ Costo: ~$50/mes (servidor pequeÃ±o)                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â–¼ Crece a 1M-10M registros
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Stage 3: Cloud Distribuido (1-2 aÃ±os)                  â”‚
â”‚  â€¢ Dask o PySpark                                       â”‚
â”‚  â€¢ Kubernetes para contenedores                         â”‚
â”‚  â€¢ Cloud Storage (S3, GCS)                              â”‚
â”‚  â€¢ 1M-10M registros/dÃ­a                                 â”‚
â”‚  â€¢ Costo: ~$500/mes                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â–¼ Crece a 10M+ registros
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Stage 4: Big Data (2+ aÃ±os)                            â”‚
â”‚  â€¢ Apache Spark                                         â”‚
â”‚  â€¢ Data Lakehouse (Delta Lake, Iceberg)                 â”‚
â”‚  â€¢ Cloud Data Warehouse (Snowflake, BigQuery)           â”‚
â”‚  â€¢ 10M+ registros/dÃ­a                                   â”‚
â”‚  â€¢ Costo: ~$2K-5K/mes                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**TÃ©cnicas de Escalabilidad Implementables:**

**1. OptimizaciÃ³n de Pandas (Corto Plazo)**
```python
# Leer solo columnas necesarias
df = pd.read_parquet('data.parquet', columns=['id', 'price', 'quantity'])

# Usar tipos de datos eficientes
df['product_id'] = df['product_id'].astype('int32')  # En vez de int64

# Procesamiento en chunks
for chunk in pd.read_csv('large.csv', chunksize=10000):
    process(chunk)
```

**2. MigraciÃ³n a Polars (Mediano Plazo)**
```python
# Mismo cÃ³digo, 10-50x mÃ¡s rÃ¡pido
import polars as pl

df = pl.read_parquet('data.parquet')
result = df.filter(pl.col('price') > 100).group_by('category').agg(pl.sum('quantity'))
```

**3. Particionamiento de Datos**
```
data/
â”œâ”€â”€ year=2024/
â”‚   â”œâ”€â”€ month=10/
â”‚   â”‚   â”œâ”€â”€ day=01/products.parquet
â”‚   â”‚   â”œâ”€â”€ day=02/products.parquet
â”‚   â”‚   â””â”€â”€ day=31/products.parquet
â”‚   â””â”€â”€ month=11/
â””â”€â”€ year=2025/
```

Ventajas:
- Solo procesar particiones relevantes
- ParalelizaciÃ³n natural
- Queries mÃ¡s rÃ¡pidas

**4. Arquitectura Modular**
```python
class DataProcessor:
    def __init__(self, engine='pandas'):
        self.engine = engine
    
    def process(self, data):
        if self.engine == 'pandas':
            return self._pandas_process(data)
        elif self.engine == 'polars':
            return self._polars_process(data)
        elif self.engine == 'spark':
            return self._spark_process(data)
```

**5. ContainerizaciÃ³n (Docker)**
```dockerfile
FROM python:3.11-slim
WORKDIR /app
COPY requirements.txt .
RUN pip install -r requirements.txt
COPY . .
CMD ["python", "run_pipeline.py"]
```

Ventajas:
- Entorno reproducible
- FÃ¡cil deployment a cloud
- Escalamiento horizontal con Kubernetes

**DecisiÃ³n de DiseÃ±o:** 
ComencÃ© con **Pandas + scripts** porque:
- âœ… Simplicidad para PoC
- âœ… Sin costos de infraestructura
- âœ… FÃ¡cil de entender y mantener
- âœ… Suficiente para volÃºmenes actuales
- âœ… CÃ³digo migratable a Polars/Spark sin reescribir lÃ³gica

---

## Ejercicio 2: ImplementaciÃ³n con Python (35 puntos)

### ğŸ¯ Resumen de ImplementaciÃ³n

| Fase | Puntos | Estado | Componentes |
|------|--------|--------|-------------|
| **Ingesta** | 10/10 | âœ… | API + CSV â†’ Parquet |
| **TransformaciÃ³n** | 15/15 | âœ… | Merges + 4 mÃ©tricas + 4 tests |
| **AutomatizaciÃ³n** | 10/10 | âœ… | Orquestador + YAML + Logging + Reportes |
| **TOTAL** | **35/35** | âœ… | **100% Completo** |

---

### 1ï¸âƒ£ Ingesta de Datos (10 puntos)

**Archivo:** `src/ingestion.py`

**Funcionalidades:**
- âœ… Descarga productos desde Fake Store API (https://fakestoreapi.com/products)
- âœ… Carga archivos CSV locales (ventas e inventario)
- âœ… ConversiÃ³n y guardado en formato Parquet
- âœ… Manejo de errores de conexiÃ³n y archivos faltantes

**CÃ³digo:**
```python
def ingest_data(api_url, sales_path, inventory_path, output_path):
    """
    Descarga datos del API, carga CSV locales y guarda en Parquet.
    
    Returns:
        tuple: (df_api, df_sales, df_inventory)
    """
    # 1. Datos del API
    response = requests.get(api_url)
    response.raise_for_status()  # Lanza error si falla
    df_api = pd.DataFrame(response.json())
    df_api.to_parquet(f"{output_path}/products.parquet")
    
    # 2. CSV locales
    df_sales = pd.read_csv(sales_path)
    df_inventory = pd.read_csv(inventory_path)
    df_sales.to_parquet(f"{output_path}/sales.parquet")
    df_inventory.to_parquet(f"{output_path}/inventory.parquet")
    
    return df_api, df_sales, df_inventory
```

**Salida Esperada:**
```
data/raw/
â”œâ”€â”€ products.parquet    (20 productos de la API)
â”œâ”€â”€ sales.parquet       (ventas histÃ³ricas)
â””â”€â”€ inventory.parquet   (stock actual)
```

---

### 2ï¸âƒ£ TransformaciÃ³n de Datos (15 puntos)

**Archivo:** `src/transformation.py`

**Funcionalidades Implementadas:**

#### âœ… Union de Datasets (Merges)
```python
# 1. Sales + Inventory
df = df_sales.merge(df_inventory, on="product_id", how="left")

# 2. Resultado + API (productos)
df = df.merge(df_api[["product_id", "title", "category", "price"]], 
              on="product_id", how="left")
```

#### âœ… MÃ©tricas de Negocio (4 requeridas)

**1. Productos con Stock CrÃ­tico**
```python
stock_critico = df[
    (df["current_stock"] < df["min_stock"])
].drop_duplicates(subset=["product_id"])
```

**2. Ventas Totales por CategorÃ­a**
```python
df["ventas_totales_categoria"] = df.groupby("category")["quantity"].transform("sum")
```

**3. Ranking de Productos MÃ¡s Vendidos**
```python
top_productos = (
    df.groupby(["product_id", "title"])["quantity"]
    .sum()
    .sort_values(ascending=False)
    .reset_index()
)
```

**4. Rentabilidad por Producto**
```python
df["total_sale_value"] = df["quantity"] * df["price"]
df["rentabilidad"] = df["total_sale_value"] - (df["cost"] * df["quantity"])
```

#### âœ… Tests de Calidad (4 implementados)

**Archivo:** `src/quality_checks.py`

**1. Precios No Negativos**
```python
precios_ok = (df["price"] >= 0).all()
```

**2. Stock Entero Positivo**
```python
stock_ok = df["current_stock"].apply(
    lambda x: isinstance(x, (int, float)) and x >= 0
).all()
```

**3. CategorÃ­as Existentes (No Nulas)**
```python
categorias_ok = df["category"].notna().all() and (df["category"] != "").all()
```

**4. Fechas VÃ¡lidas**
```python
if "sale_date" in df.columns:
    try:
        pd.to_datetime(df["sale_date"], errors="raise")
        fechas_ok = True
    except:
        fechas_ok = False
```

**Resultado de Tests:**
```python
{
    "precios_no_negativos": True,
    "stock_entero_positivo": True,
    "categorias_existentes": True,
    "fechas_validas": True
}
```

---

### 3ï¸âƒ£ AutomatizaciÃ³n (10 puntos)

#### âœ… Orquestador del Pipeline

**Archivo:** `src/orchestador.py` o `run_pipeline.py`

**CaracterÃ­sticas:**
- ğŸ”„ Ejecuta todo el flujo automÃ¡ticamente
- ğŸ“ Logging a archivo y consola
- âš™ï¸ ConfiguraciÃ³n YAML
- ğŸ›¡ï¸ Manejo de errores robusto
- ğŸ“Š GeneraciÃ³n de reportes

**Flujo de EjecuciÃ³n:**
```python
1. Cargar configuraciÃ³n (YAML)
2. Configurar logging
3. Fase 1: Ingesta â†’ Parquet
4. Fase 2: TransformaciÃ³n â†’ MÃ©tricas
5. Fase 3: Tests de calidad â†’ ValidaciÃ³n
6. Fase 4: Reportes â†’ TXT + CSV
7. Resumen final
```

#### âœ… ConfiguraciÃ³n YAML

**Archivo:** `config/pipeline_config.yaml`

```yaml
api:
  url: "https://fakestoreapi.com/products"
  timeout: 30

data_sources:
  sales_file: "data/sales.csv"
  inventory_file: "data/inventory.csv"

processing:
  output_path: "data/raw"

output:
  reports_path: "reports"

quality_checks:
  check_prices: true
  check_stock: true
  check_categories: true
  check_dates: true
```

#### âœ… Sistema de Logging

```python
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
    handlers=[
        logging.FileHandler("pipeline_execution.log", encoding='utf-8'),
        logging.StreamHandler()  # TambiÃ©n en consola
    ]
)
```

**Log Generado:**
```
2025-10-31 18:00:00,000 - INFO - ğŸš€ Iniciando pipeline de e-commerce...
2025-10-31 18:00:01,200 - INFO - âœ… Ingesta completada.
2025-10-31 18:00:02,500 - INFO - âœ… TransformaciÃ³n completada.
2025-10-31 18:00:03,100 - INFO - âœ… Tests de calidad: TODOS PASARON
2025-10-31 18:00:04,000 - INFO - ğŸ“Š Reporte generado: reports/pipeline_report_20251031_180000.txt
```

#### âœ… GeneraciÃ³n de Reportes

**Archivo:** `src/reporting.py`

**Reportes Generados:**

1. **Reporte Principal (TXT)** - 6 secciones:
   - Tests de calidad
   - EstadÃ­sticas generales
   - Stock crÃ­tico
   - Top 10 productos
   - Ventas por categorÃ­a
   - AnÃ¡lisis de rentabilidad

2. **CSVs Exportados:**
   - `stock_critico_YYYYMMDD_HHMMSS.csv`
   - `top_productos_YYYYMMDD_HHMMSS.csv`
   - `ventas_categoria_YYYYMMDD_HHMMSS.csv`
   - `datos_procesados_YYYYMMDD_HHMMSS.csv`

---

## ğŸ“¦ InstalaciÃ³n y ConfiguraciÃ³n

### Requisitos Previos

- **Python:** 3.8 o superior
- **pip:** Gestor de paquetes de Python
- **Git:** Para clonar el repositorio

### Pasos de InstalaciÃ³n

**1. Clonar el repositorio**
```bash
git clone https://github.com/tu-usuario/ecommerce-pipeline.git
cd ecommerce-pipeline
```

**2. Crear entorno virtual (recomendado)**
```bash
# Windows
python -m venv venv
venv\Scripts\activate

# Linux/Mac
python3 -m venv venv
source venv/bin/activate
```

**3. Instalar dependencias**
```bash
pip install -r requirements.txt
```

**Contenido de `requirements.txt`:**
```txt
pandas==2.0.3
requests==2.31.0
pyyaml==6.0.1
pyarrow==12.0.1
```

**4. Preparar datos de ejemplo**

Crear `data/sales.csv`:
```csv
product_id,quantity,sale_date
1,3,2024-10-10
2,5,2024-10-11
3,1,2024-10-12
4,2,2024-10-13
5,4,2024-10-14
```

Crear `data/inventory.csv`:
```csv
product_id,current_stock,min_stock
1,10,5
2,5,10
3,8,5
4,3,10
5,9,5
```

**5. Crear directorios necesarios**
```bash
mkdir -p data/raw reports
```

---

## ğŸš€ Uso del Pipeline

### EjecuciÃ³n BÃ¡sica

```bash
python run_pipeline.py
```

### EjecuciÃ³n con ConfiguraciÃ³n Personalizada

```bash
python run_pipeline.py --config config/pipeline_config_custom.yaml
```

### EjecuciÃ³n Programada (Cron)

**Linux/Mac:**
```bash
# Editar crontab
crontab -e

# Ejecutar diariamente a
